{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":84969,"databundleVersionId":10033515,"sourceType":"competition"}],"dockerImageVersionId":30822,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install copick git+https://github.com/copick/copick-utils.git git+https://github.com/copick/DeepFindET.git","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Make a copick project\n\nconfig_blob = \"\"\"{\n    \"name\": \"czii_cryoet_mlchallenge_2024\",\n    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n    \"version\": \"1.0.0\",\n\n    \"pickable_objects\": [\n        {\n            \"name\": \"apo-ferritin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"4V1W\",\n            \"label\": 1,\n            \"color\": [  0, 117, 220, 128],\n            \"radius\": 60,\n            \"map_threshold\": 0.0418\n        },\n        {\n            \"name\": \"beta-amylase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"1FA2\",\n            \"label\": 2,\n            \"color\": [153,  63,   0, 128],\n            \"radius\": 65,\n            \"map_threshold\": 0.035\n        },\n        {\n            \"name\": \"beta-galactosidase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6X1Q\",\n            \"label\": 3,\n            \"color\": [ 76,   0,  92, 128],\n            \"radius\": 90,\n            \"map_threshold\": 0.0578\n        },\n        {\n            \"name\": \"ribosome\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6EK0\",\n            \"label\": 4,\n            \"color\": [  0,  92,  49, 128],\n            \"radius\": 150,\n            \"map_threshold\": 0.0374\n        },\n        {\n            \"name\": \"thyroglobulin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6SCJ\",\n            \"label\": 5,\n            \"color\": [ 43, 206,  72, 128],\n            \"radius\": 130,\n            \"map_threshold\": 0.0278\n        },\n        {\n            \"name\": \"virus-like-particle\",\n            \"is_particle\": true,\n            \"label\": 6,\n            \"color\": [255, 204, 153, 128],\n            \"radius\": 135,\n            \"map_threshold\": 0.201\n        },\n        {\n            \"name\": \"membrane\",\n            \"is_particle\": false,\n            \"label\": 8,\n            \"color\": [100, 100, 100, 128]\n        },\n        {\n            \"name\": \"background\",\n            \"is_particle\": false,\n            \"label\": 9,\n            \"color\": [10, 150, 200, 128]\n        }\n    ],\n\n    \"overlay_root\": \"/kaggle/working/overlay\",\n\n    \"overlay_fs_args\": {\n        \"auto_mkdir\": true\n    },\n\n    \"static_root\": \"/kaggle/input/c/czii-cryo-et-object-identification/train/static\"\n}\"\"\"\n\ncopick_config_path = \"/kaggle/working/copick.config\"\noutput_overlay = \"/kaggle/working/overlay\"\n\nwith open(copick_config_path, \"w\") as f:\n    f.write(config_blob)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Setup new overlay directory\nimport os\nimport shutil\n\n# Define source and destination directories\nsource_dir = '/kaggle/input/c/czii-cryo-et-object-identification/train/overlay'\ndestination_dir = '/kaggle/working/overlay'\n\n# Walk through the source directory\nfor root, dirs, files in os.walk(source_dir):\n    # Create corresponding subdirectories in the destination\n    relative_path = os.path.relpath(root, source_dir)\n    target_dir = os.path.join(destination_dir, relative_path)\n    os.makedirs(target_dir, exist_ok=True)\n    \n    # Copy and rename each file\n    for file in files:\n        if file.startswith(\"curation_0_\"):\n            new_filename = file\n        else:\n            new_filename = f\"curation_0_{file}\"\n            \n        \n        # Define full paths for the source and destination files\n        source_file = os.path.join(root, file)\n        destination_file = os.path.join(target_dir, new_filename)\n        \n        # Copy the file with the new name\n        shutil.copy2(source_file, destination_file)\n        print(f\"Copied {source_file} to {destination_file}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepfindET.entry_points import step1\nfrom deepfindET.utils import copick_tools\nimport matplotlib.pyplot as plt\nimport copick\n\nconfig = '/kaggle/working/copick.config'\n\n# Query Tomogram\nvoxel_size = 10 \ntomogram_algorithm = 'denoised'\n\n# Output Name for the Segmentation Targets\nout_name = 'remotetargets'\nout_user_id = 'deepfindET'\nout_session_id = '0'\n\n# Read Copick Directory\ncopickRoot = copick.from_file(config)\n\n\ntrain_targets = {}\ntargets = [(obj.name, None, None, (obj.radius / voxel_size)) for obj in copickRoot.pickable_objects if obj.is_particle]\nrun_ids = None","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Generate train target information\nfor t in targets:\n    obj_name, user_id, session_id, radius = t\n    info = {\n        \"label\": copickRoot.get_object(obj_name).label,\n        \"user_id\": user_id,\n        \"session_id\": session_id,\n        \"radius\": radius,\n        \"is_particle_target\": True,\n    }\n    train_targets[obj_name] = info\n\n\n# Define segmentation target (e.g., membrane)\nseg_targets = [('membrane', None, None)]\n\n# Generate segmentation target information\nfor s in seg_targets:\n    obj_name, user_id, session_id = s\n    info = {\n        \"label\": copickRoot.get_object(obj_name).label,\n        \"user_id\": user_id,\n        \"session_id\": session_id,\n        \"radius\": None,       \n        \"is_particle_target\": False,                 \n    }\n    train_targets[obj_name] = info\n\nstep1.create_train_targets(\n    config,              # The configuration file path specifying various settings and parameters for the project.\n    train_targets,       # A dictionary containing the target information for each protein or object to be segmented.\n    run_ids,             # The list of Run-IDs for which to generate targets. None means targets for the entire project.\n    voxel_size,          # The voxel size to be used in the tomogram data.\n    tomogram_algorithm,  # The reconstruction algorithm used for the tomograms, e.g., 'wbp' (weighted back projection).\n    out_name,            # The output name for the generated segmentation targets.\n    out_user_id,         # The user ID under which the output targets will be saved.\n    out_session_id,      # The session ID associated with the output, typically used for tracking purposes.\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Option 1: Query All RunIDs\n# Retrieve all available Run-IDs from the CoPick project. This generates a list of Run-IDs by iterating over all runs in copickRoot.\nrun_ids = [run.name for run in copickRoot.runs]\n\n# Option 2: Manually Specify Specific Run\n# Define a specific Run-ID manually. This is useful for extracting volumes for a specific run.\nrunID = 'TS_6_6'\n\n# Retrieve the specific run object from CoPick using the manually specified Run-ID.\ncopick_run = copickRoot.get_run(runID)\n\n# Extract the segmentation target associated with the specified run.\n# The function get_copick_segmentation retrieves the segmentation data (e.g., target volume) based on the run object,\n# segmentation name, user ID, and session ID.\ntrain_target = copick_tools.get_copick_segmentation(\n    copick_run,                 # The run object obtained from CoPick for the specific Run-ID.\n    segmentationName='remotetargets',  # The name of the segmentation target to retrieve.\n    userID='deepfindET',        # The user ID under which the segmentation data is saved.\n    sessionID='0'               # The session ID associated with the segmentation data.\n)\n\n# Retrieve the tomogram associated with the specified Run-ID from the CoPick project.\n# The function get_copick_tomogram extracts the tomogram data, using the voxel size, algorithm, and Run-ID.\ntrain_tomogram = copick_tools.get_copick_tomogram(\n    copickRoot,                 # The root object for the CoPick project, containing all runs and associated data.\n    voxelSize=voxel_size,       # The voxel size to be used for retrieving the tomogram.\n    tomoAlgorithm='wbp',        # The reconstruction algorithm used for the tomogram, e.g., 'wbp' (weighted back projection).\n    tomoID=runID                # The specific Run-ID for which the tomogram is being retrieved.\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepfindET.entry_points import step2\n\n# Specify the directory where the training results will be saved.\ntraining_output_path = '/kaggle/working/train_results'\n\n# Set the model architecture to Residual U-Net ('res_unet'), \n# which combines U-Net with residual connections to improve training.\nmodel_name = 'res_unet'\n\n# Set to None to indicate that the model will betrained from scratch \n# without using pre-trained weights.\nmodel_pre_weights = None\n\n# Number of classes the model will predict. \n# Here, we are working with 8 different classes (6 proteins + membrane + background).\nn_class = 8\n\n# Input dimension size of the 3D volumes in voxels. Each input is a 72x72x72 voxel cube -- (72 Ã…)^3.\ndim_in = 72  # [voxels]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initiate the training of the DeepFindET 3D U-Net model.\nstep2.train_model(\n    config,                 # Configuration file with various settings for the project.\n    voxel_size,             # Voxel size used in the tomogram data.\n    tomogram_algorithm,     # Reconstruction algorithm used for the tomograms (e.g., 'wbp').\n    targets,                 # Target data for training the model.\n    training_output_path,   # Path where the training outputs will be saved.\n    model_name,             # Model architecture name ('res_unet').\n    model_pre_weights,      # Pre-trained weights (None for training from scratch).\n    n_class,                # Number of classes for segmentation (8 in this case).\n    path_valid=None,        # Path to validation data (None means internal splitting may be used).\n    dim_in=dim_in,          # Input dimension size in voxels.\n    n_sub_epoch=10,         # Number of epochs to train on tomograms prior to swapping to a new set of tomograms.\n    sample_size=3,          # Number of tomograms to extract per epoch.\n    batch_size=10,          # Batch size used during training.\n    epochs=20,              # Total number of training epochs.\n    steps_per_epoch=2,    # Number of steps per epoch.\n    n_valid=20,             # Number of validation samples.\n    model_filters=[48, 64, 128],  # Filters in the convolutional layers at each level of the U-Net.\n    model_dropout=0,        # Dropout rate (0 means no dropout applied).\n    target_name=\"remotetargets\",    # Name of the segmentation targets.\n    target_user_id=\"deepfindET\",  # User ID for the segmentation labels.\n    target_session_id=\"0\",    # Session ID associated with the labeling.\n    valid_tomo_ids=None,    # List of tomogram IDs for validation.\n    train_tomo_ids=None,     # List of tomogram IDs for training.\n    class_weights=(('apo-ferritin', 62400), ('beta-amylase', 4130), ('beta-galactosidase', 3080), ('ribosome', 1800), ('thyroglobulin', 10100), ('virus-like-particle', 8400))\n)\n# 10439:     # class_weights=(('membrane',1),('adp-mitochondrial',3000),('alkaline-phosphate',3000),('nucleosome',3000),('ribosome',750),('vault',500),('virus-like-capsid',750))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepfindET.utils import core\nimport h5py, os\n\n# Set the path to the training history file\nhistory_path = os.path.join(training_output_path, 'net_train_history.h5')\n\n# Convert the HDF5 file containing the training history into a dictionary format\n# This allows easy access to the training metrics like loss, accuracy, etc., stored during training\nhistory = core.convert_hdf5_to_dictionary(history_path)\n\n# Plot the training history to visualize the learning process\n# The plot_history function will generate curves for metrics such as training and validation loss, accuracy, etc.\ncore.plot_history(history, save_figure=False)\nplt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!ls /kaggle/working/train_results","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\n# Make a copick project\n\nconfig_blob = \"\"\"{\n    \"name\": \"czii_cryoet_mlchallenge_2024\",\n    \"description\": \"2024 CZII CryoET ML Challenge training data.\",\n    \"version\": \"1.0.0\",\n\n    \"pickable_objects\": [\n        {\n            \"name\": \"apo-ferritin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"4V1W\",\n            \"label\": 1,\n            \"color\": [  0, 117, 220, 128],\n            \"radius\": 60,\n            \"map_threshold\": 0.0418\n        },\n        {\n            \"name\": \"beta-amylase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"1FA2\",\n            \"label\": 2,\n            \"color\": [153,  63,   0, 128],\n            \"radius\": 65,\n            \"map_threshold\": 0.035\n        },\n        {\n            \"name\": \"beta-galactosidase\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6X1Q\",\n            \"label\": 3,\n            \"color\": [ 76,   0,  92, 128],\n            \"radius\": 90,\n            \"map_threshold\": 0.0578\n        },\n        {\n            \"name\": \"ribosome\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6EK0\",\n            \"label\": 4,\n            \"color\": [  0,  92,  49, 128],\n            \"radius\": 150,\n            \"map_threshold\": 0.0374\n        },\n        {\n            \"name\": \"thyroglobulin\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6SCJ\",\n            \"label\": 5,\n            \"color\": [ 43, 206,  72, 128],\n            \"radius\": 130,\n            \"map_threshold\": 0.0278\n        },\n        {\n            \"name\": \"virus-like-particle\",\n            \"is_particle\": true,\n            \"pdb_id\": \"6N4V\",            \n            \"label\": 6,\n            \"color\": [255, 204, 153, 128],\n            \"radius\": 135,\n            \"map_threshold\": 0.201\n        }\n    ],\n\n    \"overlay_root\": \"/kaggle/working/test/overlay\",\n\n    \"overlay_fs_args\": {\n        \"auto_mkdir\": true\n    },\n\n    \"static_root\": \"/kaggle/input/czii-cryo-et-object-identification/test/static\"\n}\"\"\"\n\ncopick_config_path = \"/kaggle/working/copick.config\"\n\n\nwith open(copick_config_path, \"w\") as f:\n    f.write(config_blob)\n    \n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"\n\npatch_size = 160            # Size of the input patch fed into the model for inference.\nmodel_name = 'res_unet'     # The model architecture used for training.\nfilters = [48, 64, 128]      # Number of filters for U-Net (same parameter as used for training).\ndropout = 0                 # Dropout rate applied during inference.\n\n# Path to the pre-trained model weights for the chosen architecture.\nmodel_weights = '/kaggle/working/train_results/net_weights_FINAL.h5'\n\n# Query for Tomogram\nvoxel_size = 10             # Resolution of the tomogram in voxel size.\ntomogram_algorithm = 'denoised'  # Reconstruction algorithm used for generating the tomogram\n\n# Output Segmentation Write Name\nsegmentation_name = 'predict'\nsession_id = '0'\nuser_id = 'deepfindET'\n\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Run the train DeepFindET 3D U-Net model on the copick directory.\nfrom deepfindET.entry_points import step3\nstep3.inference_tomogram_segmentation(\n    config,                                 # Copick Configuration File\n    n_class,                                # Number of classes to predict.\n    model_name,                             # The model architecture used for training.\n    model_weights,                          # Path to the pre-trained model weights for the chosen architecture.\n    patch_size,                             # Size of the input patch fed into the model for inference.\n    user_id,                                # Identifier of the user or project running the segmentation.\n    session_id,                             # Session identifier for tracking purposes.\n    segmentation_name=segmentation_name,     # Identifier for the output segmentation file.\n    voxel_size = voxel_size,                # Voxel Size of Tomogram to Run Inference On       \n    model_filters = filters,                # Number of filters for U-Net\n    model_dropout = dropout,                # Dropout rate\n    tomogram_algorithm= tomogram_algorithm, # Reconstruction algorithm used for generating the tomogram\n)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from deepfindET.entry_points import step4\n\ncopick_root = copick.from_file(config)\n\n# Session ID for the Output Picks\npicks_session_id = '0'\n\nsegmentation_session_id = '0'\n\nsegmentation_name = 'predict' \n\nmin_protein_size = 0.4  \n\npath_output = f\"{copick_root.root_overlay}/ExperimentRuns\"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"[obj.radius for obj in copick_root.pickable_objects]","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Derived from https://github.com/copick/DeepFindET/blob/main/deepfindET/entry_points/step4.py\n\nimport deepfindET.utils.copick_tools as tools\nimport deepfindET.utils.evaluate as evaluate\nimport scipy.ndimage as ndimage\nfrom tqdm import tqdm\n\n# Currently Filtering Process always finds coordinate at (cx,cy,cz) - center coordinate\n# This seems to always be at the first row, so we can remove it \nremove_index = 0\n\n# Extract Protein Coordinates from the Segmentation Masks\ndef extract_coords(pickable_object, copick_run):\n    labelmap = tools.get_copick_segmentation(copick_run, segmentation_name, user_id, segmentation_session_id)[:]\n    label = pickable_object.label\n    protein_name = pickable_object.name\n    label_objs, _ = ndimage.label(labelmap == label)\n\n    # Filter Candidates based on Object Size\n    # Get the sizes of all objects\n    object_sizes = np.bincount(label_objs.flat)\n\n    # Filter the objects based on size\n    min_object_size = 4/3 * np.pi * ((pickable_object.radius/voxel_size)**2) * min_protein_size\n    valid_objects = np.where(object_sizes > min_object_size)[0]                          \n\n    # Estimate Coordiantes from CoM for LabelMaps\n    deepFinderCoords = []\n    for object_num in tqdm(valid_objects):\n        com = ndimage.center_of_mass(label_objs == object_num)\n        swapped_com = (com[2], com[1], com[0])\n        deepFinderCoords.append(swapped_com)\n    deepFinderCoords = np.array(deepFinderCoords)   \n\n    # For some reason, consistently extracting center coordinate\n    # Remove the row with the closest index\n    deepFinderCoords = np.delete(deepFinderCoords, remove_index, axis=0)                    \n\n    # Estimate Distance Threshold Based on 1/2 of Particle Diameter\n    threshold = np.ceil(  pickable_object.radius / (voxel_size * 3) )\n\n    try: \n        # Remove Double Counted Coordinates\n        deepFinderCoords = evaluate.remove_repeated_picks(deepFinderCoords, threshold)\n\n        # Append Euler Angles to Coordinates [ Expand Dimensions from Nx3 -> Nx6 ]\n        deepFinderCoords = np.concatenate((deepFinderCoords, np.zeros(deepFinderCoords.shape)),axis=1)\n\n        # Convert from Voxel to Physical Units\n        deepFinderCoords *= voxel_size\n\n    except Exception as e:\n        print(f\"Error processing label {label} in tomo {copick_run}: {e}\")\n        deepFinderCoords = np.array([]).reshape(0,6)\n\n    # Save Picks in Copick Format / Directory \n    tools.write_copick_output(protein_name, copick_run.meta.name, deepFinderCoords, path_output, pickMethod=user_id, sessionID = picks_session_id)\n    \n    \nfor run in copick_root.runs:\n    print(f\"Run {run}\")\n    for pickable_object in copick_root.pickable_objects:\n        print(pickable_object.name)\n        if pickable_object.is_particle:\n            extract_coords(pickable_object, run)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import csv\nimport os\n\nos.listdir(\"/kaggle/input/c/czii-cryo-et-object-identification/test/static/ExperimentRuns\")\n\nresults = []\npick_id = 0\n\n# id,experiment,particle_type,x,y,z\nfor run in copick_root.runs:\n    run_id = run.meta.name\n    for particle_type in copick_root.pickable_objects:\n        picks = run.get_picks(particle_type.name, user_id=\"deepfindET\")\n        if picks:\n            picks = picks[0]\n            points = picks.points\n            for point in points:                \n                row = [pick_id, run_id, particle_type.name, point.location.x, point.location.y, point.location.z]\n                results.append(row)\n                pick_id += 1\n\nprint(f\"Found {len(results)} picks\")\n\n# Define CSV output file path\noutput_csv_path = \"/kaggle/working/submission.csv\"\n\n# Write results to CSV\nwith open(output_csv_path, mode='w', newline='') as file:\n    writer = csv.writer(file)\n    # Write header\n    writer.writerow([\"id\", \"experiment\", \"particle_type\", \"x\", \"y\", \"z\"])\n    # Write data rows\n    writer.writerows(results)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}